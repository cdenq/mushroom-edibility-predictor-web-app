<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Compare ML Models</title>

  <!--Bootstrap CSS-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <!-- Our CSS -->
  <link rel="stylesheet" type="text/css" href="../static/css/style.css">
  <!-- bootswatch -->
  <link rel="stylesheet" type="text/css" href="../static/css/bootswatch_pulse.css">
  <!-- prevents favicon.ico error -->
  <link rel="icon" href="data:,">

</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
    <a class="navbar-brand" href="/">Predict Edibility</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown"
      aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-end" id="navbarNavDropdown">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/mushroom_facts">More About Mushroom Dataset</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/compare_models">Compare Machine Learning Models</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/about">About Us</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container-fluid" style="margin-left: 10px; margin-top:30px; padding-right: 40px">
    <div>
      <h1 class = 'mt-3'>Notes on Models & Associated Graphs</h1>
      <hr>
      <h4 style="text-align:center; margin-bottom:20px"> Introduction </h4>
    </div>
    <div class="row" style="margin-left: 1px">
      <p> Shroom Zoom utilizes 4 supervised machine learning models to make its predictions:</p><br>
    </div>
      <ul>
        <li>
          Random Forest Classifer
        </li>
        <li>
          Extremely Random Forest
        </li>
        <li>
          Logistic Regression
        </li>
        <li>
          Support Vector Machine
        </li>
      </ul>
      <p  style="margin-left: 1px">
        <b>Data Preword:</b>
        The cleaned mushroom dataset comprised of possible simulated mushroom feature combinations
        (i.e. all possible undiscovered mushrooms). There are over 60k data points, 100+ features,
        (dropped to 86 in the cleaned version), and mostly categorical (eg. color: red, blue, green).
        The massive, multi-featured dataset is not linearly separable.
        <br>
        <br>
        <b>Going Forward </b> (Unsupervised Learning):
        While we used supervised learning to predict whether mushrooms were edible, we also ran unsupervised learning
        approaches (KMeans and DBSCAN) to explore a future recommendation system -- i.e., a potential next step for this project. A user could
        provide a mushroom not only to understand its edibility but also to recieve recommendations for other similar 
        edible mushrooms they may be interested in.
      </p>
    <div style="text-align:center">
      <h5 style="text-align:center; margin-bottom: 20px; margin-top:30px">
        Supervised Learning
      </h5>
    </div>
    <div class="row">
      <div class="col-sm-2" style="padding-top:50px">
        <a href="../Main/Resources/Models/erf.pkl" download>
          <p>ERF</p>
        </a>
        <a href="../Main/Resources/Models/knn.pkl" download>
          <p>KNN</p>
        </a>
        <a href="../Main/Resources/Models/lr.pkl" download>
          <p>Logistic Regression</p>
        </a>
        <a href="../Main/Resources/Models/rfc.pkl" download>
          <p>Random Forest</p>
        </a>
        <a href="../Main/Resources/Models/svm.pkl" download>
          <p>SVM</p>
        </a>
      </div>
      <div class="col-sm-3">
        <img src="../static/images/model_score_compare.png" alt="score_comp" style="height:600px; width: 600px; margin-right:100px">
      </div>
      <div class="col-sm-3">
        <div class='row'>
          <img src="../static/images/knn_classification.png" alt="score_comp" style="height:150px; width: 300px; margin-left:300px; margin-top:50px">
        </div>
        <div class="row">
          <img src="../static/images/knn_check.png" alt="score_comp" style="height:300px; width: 300px; margin-left:280px; margin-top:50px">
        </div>
      </div>
      <div>
        <p>
          As seen in the score comparisons, the RFC and ERFC are the dominant models, likely due to their ability
          to handle noisy data and strong preference for categorical data. The other models, which are much more suited
          for simple, continuous data points, provide a decently powerful model score.

          Given the nature of the data (predicting edibility of mushrooms), we want to avoid false positives (predict
          mushroom is edible, but incorrect). As a result, the biggest score we want to pay attention to is the
          precision score.
          Regarding precision, the RFC/ERFC are highest scoring top models.<br><br>

          We also ran a KNN model to check the number of clusters best identified with the model. From the testing odd
          number of
          k-clusters from 1 to 20, we get a steadily decreasing score graph as the number of clusters increased.
          However, when we
          checked it with the standard calculation (square root of n), we end up with around 116 clusters. This number
          increased
          the classification scores to just under 0.99, suggesting a that testing n-clusters will eventually increase in
          score once again.
          </p>
      </div>
    </div>
    <div style="text-align:center">
      <h5 style="text-align:center; margin-bottom: 30px">
        Unsupervised Learning
      </h5>
    </div>
    <div class="row">
      <div class="col-sm-2" style="padding-top: 20px">
        <a href="../Main/Resources/Models/Unsupervised/kmeans_mod.pkl" download>
          <p>KMeans</p>
        </a>
        <a href="../Main/Resources/Models/Unsupervised/dbscan_mod.pk1" download>
          <p>DBSCAN</p>
        </a>
        <a href="../Main/Resources/Models/Unsupervised/tsne_mod.pk1" download>
          <p>PCA/TSNE</p>
        </a>
        <a href="../Main/Resources/Models/Unsupervised/kmeans_elbow.pk1" download>
          <p>Elbow plot</p>
        </a>
      </div>
      <div class="col-sm-3">
        <img src="../static/images/kmeans_model.png" alt="kmeans_model" style="height:100%; width: 100%">
        <figcaption style="font-size: 10px; text-align: center"> <em> Kmeans Model - Mushrooms </em> </figcaption>
      </div>
      <div class="col-sm-7" style="padding-top:20px">
        <p>
          The unsupervised learning approaches
          applied here aim to
          help us understand in what ways (if possible)
          the mushrooms could be grouped for potential
          future recommendation systems.
          We could not find a reasonable number of clusters with the original data
          (no "elbow" point within 30 clusters) and so
          tried to reduce the number of features
          via PCA and TSNE. While creating such composite
          features helped identify and visualize reasonable
          numbers of clusters (~5) for kmeans and dbscan
          clustering and could still be useful for recommending similar mushrooms,
          it also means that we lost the
          ability to full explain the clusters and their
          characteristics (i.e., what characteristics of
          similiarity caused xyz mushrooms to be grouped
          together?).
        </p>
      </div>
    </div>
    <div class="row" style="margin-left:1px">
      <div>
        <h5 style="text-align:center; margin-bottom: 30px; margin-top:30px">
          More on our Models
        </h5>
      <p>
        Read below to learn more about our models methodologies! <br><br>

        <b>The Random Forest Classifier (RFC)</b>is a supervised learning algorithm that is built form
        an ensemble of decision trees. Individual decision trees take a random subset
        of features from the entire data set and make small predictions; their results are
        then merged together to give an overall prediction. One major advantage of
        RFC is that it does not assume anything about the data distribution. It can handle
        non-parametric and skewed data, as well as categorical and binary data.
        The advantages these provide in this context are clearer when we look at the resulting scores-- RFC produced
        a strong 1.0 predictive score. <br><br>
  
        <b>The Extremely Random Forest Classifier (ERFC)</b> is similar to RFC except that it takes a different random state during every decision tree selection. ERFC is typically
        a more robust model than RFC, and since RFC resulted in a 1.0 accuracy score, we were not surprised to see ERFC perform just as well. <br><br>
  
       <b> The Logistic Regression (LR)</b> model is a predictive classifier that relies on a sigmoid function. This model is perhaps the most classic supervised model used to predict binary
        results (yes/no). LR has several major assumptions: the features that are measured
        must be independent of each other (little to no collinearity) and the dataset must be largely
        separable with a sigmoid function. LR is also sensitive to noise, and, thus, gives us the lowest
        score in all departments. <br><br>
  
        <b>The Support Vector Machine (SVM)</b> is a linear classifier that attempts to draw a
        line between two groups. It calculates the optimal hyperplane that has the maximum
        margin between distinct clusters, and, thus, is heavily dependent on the distribution of the data.
        Just like LR, the SVM relies on data being linearly separable. SVM is sensitive to
        strong outliers.
      </p>
    </div>
  </div>
	<!-- Scripts -->
  <script type="text/javascript" src="../static/js/index.js"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
  <script src="assets/js/jquery.min.js"></script>
  <script src="assets/js/jquery.poptrox.min.js"></script>
  <script src="assets/js/browser.min.js"></script>
  <script src="assets/js/breakpoints.min.js"></script>
  <script src="assets/js/util.js"></script>
  <script src="assets/js/main.js"></script>

  <!-- Bootstrap Scripts -->
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>


</body>

</html>